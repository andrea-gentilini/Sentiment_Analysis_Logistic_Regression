{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#importing modules\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "nitwDsP3Qm0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "url = 'https://raw.githubusercontent.com/pieroit/corso_ml_python_youtube_pollo/master/movie_review.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "WREVcjS3QuCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning\n",
        "X = df['text']\n",
        "\n",
        "def clean_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    pronouns = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove all special characters\n",
        "    processed_text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    # Remove all single characters\n",
        "    processed_text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_text)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_text)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_text = re.sub(r'\\s+', ' ', processed_text, flags=re.I)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(processed_text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words and word not in pronouns]\n",
        "\n",
        "    # Join the words back to a string\n",
        "    processed_text = ' '.join(words)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "processed_texts = [clean_text(text) for text in X]"
      ],
      "metadata": {
        "id": "dJs--fiXQyz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = processed_texts\n",
        "y = df['tag']"
      ],
      "metadata": {
        "id": "bxgXbyYDQzO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text vectorization\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X).toarray()"
      ],
      "metadata": {
        "id": "l0UNOw18QzrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
      ],
      "metadata": {
        "id": "a9DZchHpQ8Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU56gX3JQdgx"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression \"by hand\"\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, num_epochs):\n",
        "    m = len(y)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        theta -= alpha * gradient\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Initialize parameters\n",
        "num_features = X_train.shape[1]\n",
        "theta = np.zeros(num_features)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Train the model\n",
        "theta = gradient_descent(X_train, y_train, theta, learning_rate, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "p_train = np.round(sigmoid(np.dot(X_train, theta)))\n",
        "p_test = np.round(sigmoid(np.dot(X_test, theta)))\n",
        "\n",
        "# Calculate accuracies\n",
        "acc_train = accuracy_score(y_train, p_train)\n",
        "acc_test = accuracy_score(y_test, p_test)\n",
        "\n",
        "# Print results\n",
        "print(f'Training accuracy: {acc_train}, Test accuracy: {acc_test}')"
      ],
      "metadata": {
        "id": "nv4FRLIhRBbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}